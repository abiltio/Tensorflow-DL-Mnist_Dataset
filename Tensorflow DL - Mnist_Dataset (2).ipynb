{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d790e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a779b",
   "metadata": {},
   "source": [
    "# Deep NN for MNIST Classification\n",
    "The dataset is called MNIST and refers to handwritten digit recognition.   \n",
    "\n",
    "The dataset provides 70.000 images (28x28pixels) of handwritten digit (1 digit per image)]  \n",
    "\n",
    "The goal is to write an algorithm that detecs which digit is written. Since there are only 10 digit(0-9), this is a   classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d3e3f",
   "metadata": {},
   "source": [
    "## The action Plan\n",
    "1. Prepare our data and preprocess it. Create training, validation, and test dataset\n",
    "2. Outline the model and choose activation function\n",
    "3. Set appropriate advance optimizer and loss function\n",
    "4. Make it Learn\n",
    "5. Test the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77644861",
   "metadata": {},
   "source": [
    "## Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df522c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c813943d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8962e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) \n",
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist',with_info=True, as_supervised = True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "# normally, we would like to scale our data in some way to make the result more numerically stable\n",
    "# in this case we will simply prefer to have inputs between 0 and 1\n",
    "# let's define a function called: scale, that will take an MNIST image and its label\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples,tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "\n",
    "def scale(image,label):\n",
    "    # we make sure the value is a float\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "# the method .map() allows us to apply a custom transformation to a given dataset\n",
    "# we have already decided that we will get the validation data from mnist_train, so \n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# finally, we scale and batch the test data\n",
    "# we scale it so it has the same magnitude as the train and validation\n",
    "# there is no need to shuffle it, because we won't be training on the test data\n",
    "# there would be a single batch, equal to the size of the test data\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "shuffle_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "# When we're dealing with enormous datasets, we can't shuffle all data at once\n",
    "#BUFFER_SIZE : TF take n-amount of sample data and shuffle them at once and then take another n-amount\n",
    "#IF BUFFER_SIZE >= num_samples, shuffling will happen at once (uniformly)\n",
    "#IF 1<BUFFER_SIZE<num_samples, we will be optimizing computational power\n",
    "\n",
    "validation_data = shuffle_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffle_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "#The next() loads the next element of an iterable object\n",
    "#iter() create an object which can be iterated one element at a time (e.g. in for loop or while loop) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cbf52d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f90b1",
   "metadata": {},
   "source": [
    "## Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69d8a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 #from 28x28 pixel (size of each input data)\n",
    "output_size = 10 #from 10 digits input (number range from 0-9)\n",
    "hidden_layer_size = 300 #hyperparameter (pre-set by us)\n",
    "\n",
    "#tf.keras.Sequential() function that is laying down the model (used to 'stack layers')\n",
    "#our data (from tfds) is such that each output is 28x28x1 \n",
    "#tf.keras.layers.Flatten(original shape) transform (flattens) a tensor into a vector\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "                           #tf.keras.layers.Dense(output_size) calculates the dot product of the inputs and the w and b \n",
    "                           \n",
    "    # The rectified linear activation function or ReLU for short is a piecewise linear function \n",
    "    # that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "    # As a consequence, the usage of ReLU helps to prevent \n",
    "        # - the exponential growth in the computation required to operate the neural network.\n",
    "        # - If the CNN scales in size, the computational cost of adding extra ReLUs increases linearly.\n",
    "    \n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            \n",
    "                         \n",
    "                            #we want to use 5 hidden layers\n",
    "                            tf.keras.layers.Dense(output_size, activation = 'softmax'),\n",
    "                            # when we're creating a classifier :\n",
    "                            # activation function of the output must transform the value of probability, so we use softmax\n",
    "    \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1e2ec",
   "metadata": {},
   "source": [
    "## Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4a2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer,loss,metrics) configures the model for training\n",
    "# adam : adaptive moment estimation optimizer\n",
    "# loss function for classifier normally use cross entropy\n",
    "\n",
    "# there are 3 types build-in variation \n",
    "# binnary_crossentropy() : Binary encoding\n",
    "# categorical_crossentropy() : Expects that you've one-hot encoded the targets\n",
    "# sparse_categorical_crossentropy() : applies one-hot encoding\n",
    "\n",
    "# The output and target layers should have matching forms \n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fe921",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b765f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "360/360 - 3s - loss: 0.2688 - accuracy: 0.9179 - val_loss: 0.1162 - val_accuracy: 0.9647 - 3s/epoch - 9ms/step\n",
      "Epoch 2/5\n",
      "360/360 - 2s - loss: 0.1044 - accuracy: 0.9683 - val_loss: 0.1136 - val_accuracy: 0.9640 - 2s/epoch - 7ms/step\n",
      "Epoch 3/5\n",
      "360/360 - 2s - loss: 0.0734 - accuracy: 0.9776 - val_loss: 0.0767 - val_accuracy: 0.9765 - 2s/epoch - 7ms/step\n",
      "Epoch 4/5\n",
      "360/360 - 2s - loss: 0.0568 - accuracy: 0.9823 - val_loss: 0.0766 - val_accuracy: 0.9773 - 2s/epoch - 7ms/step\n",
      "Epoch 5/5\n",
      "360/360 - 2s - loss: 0.0453 - accuracy: 0.9860 - val_loss: 0.0736 - val_accuracy: 0.9798 - 2s/epoch - 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab7620e190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets),verbose =2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251af1d3",
   "metadata": {},
   "source": [
    "What Happens inside an Epochs \n",
    "1. At the beginning of the epochs, the training loss will be set to 0\n",
    "2. The algorithm will iterate over a preset number of batches, all from train_data\n",
    "3. The weights and biases will be updated as many times as there are batches \n",
    "4. We will get a value for the loss fucntion, indicating how the training is going \n",
    "5. We will see the accuracy \n",
    "6. At the end of the epoch, the algorithm will forward propagate the whole validation set \n",
    "* When we reach the maximum number of epochs the training will be over\n",
    "\n",
    "`val_accuracy` = The true accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca30e4e",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894424d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0942 - accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed48dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09. Test accuracy: 97.55%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e65d1b",
   "metadata": {},
   "source": [
    "## Result \n",
    "The final result give us 97.55% test accuracy, by using 5 hidden layers with 'relu' activication function, with :  \n",
    "`hidden_layer_size` = 300  \n",
    "`BUFFER_SIZE` = 10000  \n",
    "`BATCH_SIZE` = 150  \n",
    "`NUM_EPOCHS` = 5  \n",
    "  \n",
    "\n",
    "However we still can improve our model by using different methods or hyperparameters size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6b2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
